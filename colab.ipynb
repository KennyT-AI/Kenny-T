{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KennyT-AI/Kenny-T/blob/main/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7aJhsgLAWvO"
      },
      "source": [
        "# Style-Bert-VITS2 (ver 2.7.0) のGoogle Colabでの学習\n",
        "\n",
        "Google Colab上でStyle-Bert-VITS2の学習を行うことができます。\n",
        "\n",
        "このnotebookでは、通常使用ではあなたのGoogle Driveにフォルダ`Style-Bert-VITS2`を作り、その内部での作業を行います。他のフォルダには触れません。\n",
        "Google Driveを使わない場合は、初期設定のところで適切なパスを指定してください。\n",
        "\n",
        "## 流れ\n",
        "\n",
        "### 学習を最初からやりたいとき\n",
        "上から順に実行していけばいいです。音声合成に必要なファイルはGoogle Driveの`Style-Bert-VITS2/model_assets/`に保存されます。また、途中経過も`Style-Bert-VITS2/Data/`に保存されるので、学習を中断したり、途中から再開することもできます。\n",
        "\n",
        "### 学習を途中から再開したいとき\n",
        "0と1を行い、3の前処理は飛ばして、4から始めてください。スタイル分け5は、学習が終わったら必要なら行ってください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-gAIubBAWvQ"
      },
      "source": [
        "## 0. 環境構築\n",
        "\n",
        "Style-Bert-VITS2の環境をcolab上に構築します。ランタイムがT4等のGPUバックエンドになっていることを確認し、実行してください。\n",
        "\n",
        "**注意**: このセルを実行した後に「セッションがクラッシュしました」「不明な理由により、セッションがクラッシュしました。」等の警告が出ますが、**無視してそのまま先へ**進んでください。（一度ランタイムを再起動させてnumpy<2を強制させるため `exit()` を呼んでいることからの措置です。）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GNj8JyDAlm2",
        "outputId": "f6ba4c62-d39a-4086-f2f1-d5aae7ebf413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading uv 0.8.17 x86_64-unknown-linux-gnu\n",
            "no checksums to verify\n",
            "installing to /usr/local/bin\n",
            "  uv\n",
            "  uvx\n",
            "everything's installed!\n",
            "Cloning into 'Style-Bert-VITS2'...\n",
            "remote: Enumerating objects: 6852, done.\u001b[K\n",
            "remote: Counting objects: 100% (3966/3966), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1052/1052), done.\u001b[K\n",
            "remote: Total 6852 (delta 3075), reused 2914 (delta 2914), pack-reused 2886 (from 2)\u001b[K\n",
            "Receiving objects: 100% (6852/6852), 14.62 MiB | 12.53 MiB/s, done.\n",
            "Resolving deltas: 100% (4595/4595), done.\n",
            "/content/Style-Bert-VITS2\n",
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mResolved \u001b[1m178 packages\u001b[0m \u001b[2min 2.46s\u001b[0m\u001b[0m\n",
            "\u001b[2mPrepared \u001b[1m46 packages\u001b[0m \u001b[2min 6.44s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 86ms\u001b[0m\u001b[0m\n",
            "\u001b[2mInstalled \u001b[1m46 packages\u001b[0m \u001b[2min 339ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1masteroid-filterbanks\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcmudict\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcn2an\u001b[0m\u001b[2m==0.5.23\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcoloredlogs\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcolorlog\u001b[0m\u001b[2m==6.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdistance\u001b[0m\u001b[2m==0.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdocopt\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mg2p-en\u001b[0m\u001b[2m==2.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhumanfriendly\u001b[0m\u001b[2m==10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhyperpyyaml\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjulius\u001b[0m\u001b[2m==0.2.7\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mlibrosa\u001b[0m\u001b[2m==0.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlibrosa\u001b[0m\u001b[2m==0.9.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlightning\u001b[0m\u001b[2m==2.5.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlightning-utilities\u001b[0m\u001b[2m==0.15.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mloguru\u001b[0m\u001b[2m==0.7.3\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnltk\u001b[0m\u001b[2m==3.9.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnltk\u001b[0m\u001b[2m==3.8.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnum2words\u001b[0m\u001b[2m==0.5.14\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1monnx\u001b[0m\u001b[2m==1.19.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1monnxconverter-common\u001b[0m\u001b[2m==1.16.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1monnxruntime\u001b[0m\u001b[2m==1.22.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1monnxruntime-gpu\u001b[0m\u001b[2m==1.22.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1monnxsim-prebuilt\u001b[0m\u001b[2m==0.4.36.post1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1moptuna\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprimepy\u001b[0m\u001b[2m==1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mproces\u001b[0m\u001b[2m==0.1.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyannote-audio\u001b[0m\u001b[2m==3.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyannote-core\u001b[0m\u001b[2m==5.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyannote-database\u001b[0m\u001b[2m==5.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyannote-metrics\u001b[0m\u001b[2m==3.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyannote-pipeline\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyloudnorm\u001b[0m\u001b[2m==0.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyopenjtalk-dict\u001b[0m\u001b[2m==0.3.4.dev2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpypinyin\u001b[0m\u001b[2m==0.55.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytorch-lightning\u001b[0m\u001b[2m==2.5.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytorch-metric-learning\u001b[0m\u001b[2m==2.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyworld-prebuilt\u001b[0m\u001b[2m==0.3.5.post1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mresampy\u001b[0m\u001b[2m==0.4.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mruamel-yaml\u001b[0m\u001b[2m==0.18.15\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mruamel-yaml-clib\u001b[0m\u001b[2m==0.2.12\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msemver\u001b[0m\u001b[2m==3.0.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mspeechbrain\u001b[0m\u001b[2m==1.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtensorboardx\u001b[0m\u001b[2m==2.6.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-audiomentations\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-pitch-shift\u001b[0m\u001b[2m==1.2.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchmetrics\u001b[0m\u001b[2m==1.8.2\u001b[0m\n",
            "09-17 08:59:07 |  INFO  | initialize.py:19 | Downloading deberta-v2-large-japanese-char-wwm pytorch_model.bin\n",
            "pytorch_model.bin: 100% 1.32G/1.32G [00:20<00:00, 63.5MB/s]\n",
            "09-17 08:59:29 |  INFO  | initialize.py:19 | Downloading deberta-v2-large-japanese-char-wwm-onnx model_fp16.onnx\n",
            "model_fp16.onnx: 100% 653M/653M [00:07<00:00, 86.1MB/s]\n",
            "09-17 08:59:37 |  INFO  | initialize.py:19 | Downloading chinese-roberta-wwm-ext-large pytorch_model.bin\n",
            "pytorch_model.bin: 100% 1.31G/1.31G [00:13<00:00, 99.9MB/s]\n",
            "09-17 08:59:50 |  INFO  | initialize.py:19 | Downloading chinese-roberta-wwm-ext-large-onnx model_fp16.onnx\n",
            "model_fp16.onnx: 100% 599M/599M [00:08<00:00, 70.0MB/s]\n",
            "09-17 09:00:00 |  INFO  | initialize.py:19 | Downloading deberta-v3-large spm.model\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 4.80MB/s]\n",
            "09-17 09:00:01 |  INFO  | initialize.py:19 | Downloading deberta-v3-large pytorch_model.bin\n",
            "pytorch_model.bin: 100% 874M/874M [00:13<00:00, 66.3MB/s]\n",
            "09-17 09:00:15 |  INFO  | initialize.py:19 | Downloading deberta-v3-large-onnx spm.model\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 4.92MB/s]\n",
            "09-17 09:00:16 |  INFO  | initialize.py:19 | Downloading deberta-v3-large-onnx model_fp16.onnx\n",
            "model_fp16.onnx: 100% 864M/864M [00:11<00:00, 77.4MB/s]\n",
            "09-17 09:00:28 |  INFO  | initialize.py:27 | Downloading wavlm-base-plus pytorch_model.bin\n",
            "pytorch_model.bin: 100% 378M/378M [00:06<00:00, 54.3MB/s]\n",
            "09-17 09:00:35 |  INFO  | initialize.py:36 | Downloading pretrained G_0.safetensors\n",
            "G_0.safetensors: 100% 234M/234M [00:07<00:00, 32.0MB/s]\n",
            "09-17 09:00:43 |  INFO  | initialize.py:36 | Downloading pretrained D_0.safetensors\n",
            "D_0.safetensors: 100% 187M/187M [00:03<00:00, 47.9MB/s]\n",
            "09-17 09:00:47 |  INFO  | initialize.py:36 | Downloading pretrained DUR_0.safetensors\n",
            "DUR_0.safetensors: 100% 2.42M/2.42M [00:00<00:00, 3.30MB/s]\n",
            "09-17 09:00:49 |  INFO  | initialize.py:47 | Downloading JP-Extra pretrained G_0.safetensors\n",
            "G_0.safetensors: 100% 293M/293M [00:08<00:00, 35.7MB/s]\n",
            "09-17 09:00:57 |  INFO  | initialize.py:47 | Downloading JP-Extra pretrained D_0.safetensors\n",
            "D_0.safetensors: 100% 187M/187M [00:02<00:00, 78.3MB/s]\n",
            "09-17 09:01:01 |  INFO  | initialize.py:47 | Downloading JP-Extra pretrained WD_0.safetensors\n",
            "WD_0.safetensors: 100% 4.70M/4.70M [00:00<00:00, 6.56MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PATH\"] += \":/root/.cargo/bin\"\n",
        "\n",
        "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "!git clone https://github.com/litagin02/Style-Bert-VITS2.git\n",
        "%cd Style-Bert-VITS2/\n",
        "!uv pip install --system -r requirements-colab.txt --no-progress\n",
        "!python initialize.py --skip_default_models\n",
        "\n",
        "exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5z1nzkvAWvR",
        "outputId": "dce5c935-4f4b-4a59-ac18-fb5bd72714fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Google driveを使う方はこちらを実行してください。\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU9apXzcAWvR"
      },
      "source": [
        "## 1. 初期設定\n",
        "\n",
        "学習とその結果を保存するディレクトリ名を指定します。\n",
        "Google driveの場合はそのまま実行、カスタマイズしたい方は変更して実行してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gO3OwZV1AWvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38cbfab-a278-4536-e8c2-b280099679c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Style-Bert-VITS2\n"
          ]
        }
      ],
      "source": [
        "# 作業ディレクトリを移動\n",
        "%cd /content/Style-Bert-VITS2/\n",
        "\n",
        "# 学習に必要なファイルや途中経過が保存されるディレクトリ\n",
        "dataset_root = \"/content/drive/MyDrive/Style-Bert-VITS2/Data\"\n",
        "\n",
        "# 学習結果（音声合成に必要なファイルたち）が保存されるディレクトリ\n",
        "assets_root = \"/content/drive/MyDrive/Style-Bert-VITS2/model_assets\"\n",
        "\n",
        "import yaml\n",
        "\n",
        "\n",
        "with open(\"configs/paths.yml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.dump({\"dataset_root\": dataset_root, \"assets_root\": assets_root}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA_yLeezAWvS"
      },
      "source": [
        "## 2. 学習に使うデータ準備\n",
        "\n",
        "すでに音声ファイル（1ファイル2-12秒程度）とその書き起こしデータがある場合は2.2を、ない場合は2.1を実行してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s9gOnTCAWvS"
      },
      "source": [
        "### 2.1 音声ファイルからのデータセットの作成（ある人はスキップ可）\n",
        "\n",
        "音声ファイル（1ファイル2-12秒程度）とその書き起こしのデータセットを持っていない方は、（日本語の）音声ファイルのみから以下の手順でデータセットを作成することができます。Google drive上の`Style-Bert-VITS2/inputs/`フォルダに音声ファイル（wavやmp3等の通常の音声ファイル形式、1ファイルでも複数ファイルでも可）を置いて、下を実行すると、データセットが作られ、自動的に正しい場所へ配置されます。\n",
        "\n",
        "**2024-06-02のVer 2.5以降**、`inputs/`フォルダにサブフォルダを2個以上作ってそこへ音声ファイルをスタイルに応じて振り分けて置くと、学習の際にサブディレクトリに応じたスタイルが自動的に作成されます。デフォルトスタイルのみでよい場合や手動でスタイルを後で作成する場合は`inputs/`直下へ入れれば大丈夫です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fXCTPuiAWvS",
        "outputId": "95ed0939-48bd-476a-d6d3-321a62667e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09-17 09:58:52 |  INFO  | transcribe.py:157 | Found 11 WAV files\n",
            "09-17 09:58:52 |  INFO  | transcribe.py:204 | Loading HF Whisper model (openai/whisper-large-v2)\n",
            "\r  0%|          | 0/11 [00:00<?, ?it/s]2025-09-17 09:58:55.946006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758103135.965895   16504 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758103135.971884   16504 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758103135.987356   16504 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103135.987381   16504 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103135.987384   16504 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103135.987388   16504 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-17 09:58:55.992002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Fetching 1 files:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "preprocessor_config.json: 185kB [00:00, 195MB/s]\n",
            "\n",
            "Fetching 1 files: 100% 1/1 [00:00<00:00,  3.89it/s]\n",
            "\n",
            "tokenizer_config.json: 283kB [00:00, 330MB/s]\n",
            "\n",
            "vocab.json: 836kB [00:00, 71.9MB/s]\n",
            "\n",
            "tokenizer.json: 2.48MB [00:00, 110MB/s]\n",
            "\n",
            "merges.txt: 494kB [00:00, 92.9MB/s]\n",
            "\n",
            "normalizer.json: 52.7kB [00:00, 86.2MB/s]\n",
            "\n",
            "added_tokens.json: 34.6kB [00:00, 89.2MB/s]\n",
            "\n",
            "special_tokens_map.json: 2.19kB [00:00, 10.1MB/s]\n",
            "09-17 09:59:04 |  INFO  | transcribe.py:70 | generate_kwargs: {'language': 'ja', 'do_sample': False, 'num_beams': 1, 'no_repeat_ngram_size': 10}, loading pipeline...\n",
            "\n",
            "config.json: 1.99kB [00:00, 12.7MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "\n",
            "model.safetensors:   0% 0.00/6.17G [00:00<?, ?B/s]\u001b[A\n",
            "model.safetensors:   0% 110k/6.17G [00:02<32:42:01, 52.4kB/s]\u001b[A\n",
            "model.safetensors:   0% 585k/6.17G [00:02<6:22:44, 269kB/s]  \u001b[A\n",
            "model.safetensors:   0% 21.6M/6.17G [00:03<08:29, 12.1MB/s]\u001b[A\n",
            "model.safetensors:   1% 34.5M/6.17G [00:04<09:00, 11.4MB/s]\u001b[A\n",
            "model.safetensors:   1% 49.5M/6.17G [00:04<06:18, 16.2MB/s]\u001b[A\n",
            "model.safetensors:   2% 117M/6.17G [00:05<02:17, 44.1MB/s] \u001b[A\n",
            "model.safetensors:   2% 141M/6.17G [00:05<02:13, 45.1MB/s]\u001b[A\n",
            "model.safetensors:   3% 208M/6.17G [00:06<01:55, 51.4MB/s]\u001b[A\n",
            "model.safetensors:   4% 234M/6.17G [00:07<01:48, 54.8MB/s]\u001b[A\n",
            "model.safetensors:   5% 301M/6.17G [00:07<01:15, 77.3MB/s]\u001b[A\n",
            "model.safetensors:   6% 368M/6.17G [00:08<01:09, 83.2MB/s]\u001b[A\n",
            "model.safetensors:   7% 435M/6.17G [00:09<01:15, 75.9MB/s]\u001b[A\n",
            "model.safetensors:   7% 455M/6.17G [00:14<04:21, 21.8MB/s]\u001b[A\n",
            "model.safetensors:   8% 522M/6.17G [00:14<02:44, 34.4MB/s]\u001b[A\n",
            "model.safetensors:  10% 599M/6.17G [00:15<01:59, 46.7MB/s]\u001b[A\n",
            "model.safetensors:  11% 652M/6.17G [00:15<01:33, 59.0MB/s]\u001b[A\n",
            "model.safetensors:  12% 719M/6.17G [00:16<01:19, 69.0MB/s]\u001b[A\n",
            "model.safetensors:  12% 755M/6.17G [00:16<01:07, 80.4MB/s]\u001b[A\n",
            "model.safetensors:  13% 822M/6.17G [00:17<01:05, 81.6MB/s]\u001b[A\n",
            "model.safetensors:  14% 889M/6.17G [00:17<00:54, 96.7MB/s]\u001b[A\n",
            "model.safetensors:  15% 956M/6.17G [00:18<00:52, 100MB/s] \u001b[A\n",
            "model.safetensors:  17% 1.02G/6.17G [00:18<00:46, 111MB/s]\u001b[A\n",
            "model.safetensors:  18% 1.09G/6.17G [00:19<00:43, 117MB/s]\u001b[A\n",
            "model.safetensors:  19% 1.15G/6.17G [00:24<02:42, 30.9MB/s]\u001b[A\n",
            "model.safetensors:  20% 1.23G/6.17G [00:25<01:55, 42.8MB/s]\u001b[A\n",
            "model.safetensors:  21% 1.30G/6.17G [00:26<01:33, 52.1MB/s]\u001b[A\n",
            "model.safetensors:  22% 1.36G/6.17G [00:26<01:17, 62.4MB/s]\u001b[A\n",
            "model.safetensors:  23% 1.43G/6.17G [00:26<01:00, 78.1MB/s]\u001b[A\n",
            "model.safetensors:  24% 1.50G/6.17G [00:27<00:55, 83.6MB/s]\u001b[A\n",
            "model.safetensors:  25% 1.56G/6.17G [00:27<00:45, 101MB/s] \u001b[A\n",
            "model.safetensors:  26% 1.63G/6.17G [00:28<00:46, 96.8MB/s]\u001b[A\n",
            "model.safetensors:  27% 1.70G/6.17G [00:28<00:34, 128MB/s] \u001b[A\n",
            "model.safetensors:  29% 1.76G/6.17G [00:29<00:41, 107MB/s]\u001b[A\n",
            "model.safetensors:  30% 1.83G/6.17G [00:30<00:41, 104MB/s]\u001b[A\n",
            "model.safetensors:  31% 1.90G/6.17G [00:30<00:36, 118MB/s]\u001b[A\n",
            "model.safetensors:  32% 1.97G/6.17G [00:31<00:31, 133MB/s]\u001b[A\n",
            "model.safetensors:  33% 2.03G/6.17G [00:34<01:23, 49.7MB/s]\u001b[A\n",
            "model.safetensors:  34% 2.10G/6.17G [00:35<01:10, 57.5MB/s]\u001b[A\n",
            "model.safetensors:  35% 2.15G/6.17G [00:35<00:54, 73.8MB/s]\u001b[A\n",
            "model.safetensors:  36% 2.22G/6.17G [00:39<01:46, 37.1MB/s]\u001b[A\n",
            "model.safetensors:  37% 2.29G/6.17G [00:39<01:15, 51.6MB/s]\u001b[A\n",
            "model.safetensors:  38% 2.32G/6.17G [00:39<01:15, 51.4MB/s]\u001b[A\n",
            "model.safetensors:  38% 2.37G/6.17G [00:40<01:03, 60.0MB/s]\u001b[A\n",
            "model.safetensors:  40% 2.44G/6.17G [00:40<00:47, 78.2MB/s]\u001b[A\n",
            "model.safetensors:  41% 2.51G/6.17G [00:45<01:53, 32.2MB/s]\u001b[A\n",
            "model.safetensors:  42% 2.57G/6.17G [00:45<01:24, 42.4MB/s]\u001b[A\n",
            "model.safetensors:  43% 2.64G/6.17G [00:49<01:55, 30.6MB/s]\u001b[A\n",
            "model.safetensors:  44% 2.71G/6.17G [00:51<01:58, 29.4MB/s]\u001b[A\n",
            "model.safetensors:  45% 2.78G/6.17G [00:55<02:16, 24.8MB/s]\u001b[A\n",
            "model.safetensors:  46% 2.81G/6.17G [00:55<01:56, 28.9MB/s]\u001b[A\n",
            "model.safetensors:  47% 2.88G/6.17G [00:56<01:21, 40.4MB/s]\u001b[A\n",
            "model.safetensors:  48% 2.94G/6.17G [00:56<01:04, 50.3MB/s]\u001b[A\n",
            "model.safetensors:  49% 3.01G/6.17G [00:57<00:50, 63.1MB/s]\u001b[A\n",
            "model.safetensors:  50% 3.08G/6.17G [00:57<00:42, 72.7MB/s]\u001b[A\n",
            "model.safetensors:  51% 3.14G/6.17G [00:58<00:35, 84.5MB/s]\u001b[A\n",
            "model.safetensors:  52% 3.21G/6.17G [00:58<00:30, 97.6MB/s]\u001b[A\n",
            "model.safetensors:  53% 3.25G/6.17G [00:59<00:27, 108MB/s] \u001b[A\n",
            "model.safetensors:  54% 3.32G/6.17G [00:59<00:24, 118MB/s]\u001b[A\n",
            "model.safetensors:  55% 3.39G/6.17G [01:00<00:26, 104MB/s]\u001b[A\n",
            "model.safetensors:  56% 3.46G/6.17G [01:00<00:22, 120MB/s]\u001b[A\n",
            "model.safetensors:  57% 3.53G/6.17G [01:04<01:00, 43.7MB/s]\u001b[A\n",
            "model.safetensors:  58% 3.60G/6.17G [01:05<00:54, 47.1MB/s]\u001b[A\n",
            "model.safetensors:  59% 3.66G/6.17G [01:07<00:59, 42.1MB/s]\u001b[A\n",
            "model.safetensors:  60% 3.73G/6.17G [01:08<00:50, 48.0MB/s]\u001b[A\n",
            "model.safetensors:  62% 3.80G/6.17G [01:08<00:36, 64.9MB/s]\u001b[A\n",
            "model.safetensors:  63% 3.87G/6.17G [01:10<00:43, 52.5MB/s]\u001b[A\n",
            "model.safetensors:  64% 3.93G/6.17G [01:11<00:33, 66.5MB/s]\u001b[A\n",
            "model.safetensors:  65% 4.00G/6.17G [01:11<00:30, 72.4MB/s]\u001b[A\n",
            "model.safetensors:  66% 4.07G/6.17G [01:14<00:41, 50.7MB/s]\u001b[A\n",
            "model.safetensors:  67% 4.13G/6.17G [01:14<00:31, 63.8MB/s]\u001b[A\n",
            "model.safetensors:  68% 4.20G/6.17G [01:18<00:52, 37.4MB/s]\u001b[A\n",
            "model.safetensors:  69% 4.27G/6.17G [01:18<00:37, 50.8MB/s]\u001b[A\n",
            "model.safetensors:  70% 4.33G/6.17G [01:22<00:56, 32.6MB/s]\u001b[A\n",
            "model.safetensors:  72% 4.44G/6.17G [01:22<00:35, 49.3MB/s]\u001b[A\n",
            "model.safetensors:  73% 4.50G/6.17G [01:28<01:03, 26.5MB/s]\u001b[A\n",
            "model.safetensors:  74% 4.57G/6.17G [01:28<00:44, 36.1MB/s]\u001b[A\n",
            "model.safetensors:  75% 4.60G/6.17G [01:28<00:41, 38.1MB/s]\u001b[A\n",
            "model.safetensors:  76% 4.67G/6.17G [01:29<00:28, 53.1MB/s]\u001b[A\n",
            "model.safetensors:  77% 4.73G/6.17G [01:29<00:23, 60.2MB/s]\u001b[A\n",
            "model.safetensors:  78% 4.80G/6.17G [01:30<00:17, 77.8MB/s]\u001b[A\n",
            "model.safetensors:  78% 4.83G/6.17G [01:30<00:17, 77.8MB/s]\u001b[A\n",
            "model.safetensors:  79% 4.90G/6.17G [01:31<00:14, 86.1MB/s]\u001b[A\n",
            "model.safetensors:  80% 4.97G/6.17G [01:31<00:10, 117MB/s] \u001b[A\n",
            "model.safetensors:  82% 5.03G/6.17G [01:32<00:10, 105MB/s]\u001b[A\n",
            "model.safetensors:  83% 5.10G/6.17G [01:32<00:09, 108MB/s]\u001b[A\n",
            "model.safetensors:  84% 5.17G/6.17G [01:33<00:08, 112MB/s]\u001b[A\n",
            "model.safetensors:  85% 5.23G/6.17G [01:33<00:07, 133MB/s]\u001b[A\n",
            "model.safetensors:  86% 5.30G/6.17G [01:34<00:06, 127MB/s]\u001b[A\n",
            "model.safetensors:  87% 5.37G/6.17G [01:36<00:12, 62.6MB/s]\u001b[A\n",
            "model.safetensors:  88% 5.44G/6.17G [01:36<00:08, 84.8MB/s]\u001b[A\n",
            "model.safetensors:  89% 5.50G/6.17G [01:41<00:18, 36.0MB/s]\u001b[A\n",
            "model.safetensors:  90% 5.57G/6.17G [01:41<00:14, 43.0MB/s]\u001b[A\n",
            "model.safetensors:  91% 5.64G/6.17G [01:42<00:09, 59.1MB/s]\u001b[A\n",
            "model.safetensors:  92% 5.70G/6.17G [01:42<00:06, 70.9MB/s]\u001b[A\n",
            "model.safetensors:  93% 5.77G/6.17G [01:42<00:04, 84.4MB/s]\u001b[A\n",
            "model.safetensors:  95% 5.84G/6.17G [01:43<00:03, 92.2MB/s]\u001b[A\n",
            "model.safetensors:  96% 5.91G/6.17G [01:44<00:03, 82.9MB/s]\u001b[A\n",
            "model.safetensors:  97% 5.97G/6.17G [01:45<00:02, 77.7MB/s]\u001b[A\n",
            "model.safetensors:  98% 6.04G/6.17G [01:46<00:01, 88.3MB/s]\u001b[A\n",
            "model.safetensors:  99% 6.11G/6.17G [01:46<00:00, 99.0MB/s]\u001b[A\n",
            "model.safetensors: 100% 6.17G/6.17G [01:47<00:00, 57.7MB/s]\n",
            "\n",
            "generation_config.json: 4.29kB [00:00, 21.0MB/s]\n",
            "\n",
            "Fetching 1 files: 100% 1/1 [00:00<00:00, 6594.82it/s]\n",
            "Device set to use cuda\n",
            "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
            "09-17 10:01:22 |  INFO  | transcribe.py:81 | Loaded pipeline\n",
            "100%|##########| 11/11 [02:37<00:00, 14.35s/it]\n"
          ]
        }
      ],
      "source": [
        "# 元となる音声ファイル（wav形式）を入れるディレクトリ\n",
        "input_dir = \"/content/drive/MyDrive/Style-Bert-VITS2/inputs\"\n",
        "# モデル名（話者名）を入力\n",
        "model_name = \"aoyama\"\n",
        "\n",
        "# こういうふうに書き起こして欲しいという例文（句読点の入れ方・笑い方や固有名詞等）\n",
        "initial_prompt = \"こんにちは。元気、ですかー？ふふっ、私は……ちゃんと元気だよ！\"\n",
        "\n",
        "!python transcribe.py --model_name {model_name} --initial_prompt {initial_prompt} --use_hf_whisper --hf_repo_id \"openai/whisper-large-v2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584e1f26-0af3-4a9d-e659-19a80f2c5841",
        "id": "rCSQ1KxM_Qj7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09-17 09:13:34 |  INFO  | slice.py:167 | Found 10 audio files.\n",
            "Using cache found in /root/.cache/torch/hub/litagin02_silero-vad_master\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]Using cache found in /root/.cache/torch/hub/litagin02_silero-vad_master\n",
            "/root/.cache/torch/hub/litagin02_silero-vad_master/utils_vad.py:127: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  audio_backends = torchaudio.list_audio_backends()\n",
            "Using cache found in /root/.cache/torch/hub/litagin02_silero-vad_master\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "Using cache found in /root/.cache/torch/hub/litagin02_silero-vad_master\n",
            "09-17 09:13:36 | ERROR  | slice.py:213 | Error processing /content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie3.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie3.m4a': Format not recognised.\n",
            " 10%|#         | 1/10 [00:00<00:07,  1.18it/s]09-17 09:13:36 | ERROR  | slice.py:213 | Error processing /content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie2.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie2.m4a': Format not recognised.\n",
            "09-17 09:13:36 | ERROR  | slice.py:213 | Error processing /content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie5.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie5.m4a': Format not recognised.\n",
            " 30%|###       | 3/10 [00:01<00:01,  3.61it/s]09-17 09:13:37 | ERROR  | slice.py:213 | Error processing /content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie.m4a': Format not recognised.\n",
            " 40%|####      | 4/10 [00:01<00:02,  2.94it/s]09-17 09:13:37 | ERROR  | slice.py:213 | Error processing /content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie4.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie4.m4a': Format not recognised.\n",
            " 50%|#####     | 5/10 [00:01<00:01,  3.79it/s]09-17 09:13:37 | ERROR  | slice.py:213 | Error processing /content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie6.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie6.m4a': Format not recognised.\n",
            " 60%|######    | 6/10 [00:01<00:00,  4.64it/s]09-17 09:13:37 | ERROR  | slice.py:213 | Error processing /content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie7.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie7.m4a': Format not recognised.\n",
            " 70%|#######   | 7/10 [00:02<00:00,  3.01it/s]09-17 09:13:37 | ERROR  | slice.py:213 | Error processing /content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie9.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie9.m4a': Format not recognised.\n",
            " 80%|########  | 8/10 [00:02<00:00,  3.79it/s]09-17 09:13:38 | ERROR  | slice.py:213 | Error processing /content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie10.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie10.m4a': Format not recognised.\n",
            "09-17 09:13:38 | ERROR  | slice.py:213 | Error processing /content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie8.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie8.m4a': Format not recognised.\n",
            "100%|##########| 10/10 [00:02<00:00,  3.80it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Style-Bert-VITS2/slice.py\", line 263, in <module>\n",
            "    raise RuntimeError(error_str)\n",
            "RuntimeError: Error slicing some files:\n",
            "/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie3.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie3.m4a': Format not recognised.\n",
            "/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie2.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie2.m4a': Format not recognised.\n",
            "/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie5.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie5.m4a': Format not recognised.\n",
            "/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie.m4a': Format not recognised.\n",
            "/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie4.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie4.m4a': Format not recognised.\n",
            "/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie6.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie6.m4a': Format not recognised.\n",
            "/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie7.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie7.m4a': Format not recognised.\n",
            "/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie9.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie9.m4a': Format not recognised.\n",
            "/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie10.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie10.m4a': Format not recognised.\n",
            "/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie8.m4a: Error opening '/content/drive/MyDrive/Style-Bert-VITS2/inputs/My Movie8.m4a': Format not recognised.\n",
            "09-17 09:13:41 |  INFO  | transcribe.py:157 | Found 0 WAV files\n",
            "09-17 09:13:41 |WARNING | transcribe.py:159 | No WAV files found in /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/raw\n"
          ]
        }
      ],
      "source": [
        "# 元となる音声ファイル（wav形式）を入れるディレクトリ\n",
        "input_dir = \"/content/drive/MyDrive/Style-Bert-VITS2/inputs\"\n",
        "# モデル名（話者名）を入力\n",
        "model_name = \"your_model_name\"\n",
        "\n",
        "# こういうふうに書き起こして欲しいという例文（句読点の入れ方・笑い方や固有名詞等）\n",
        "initial_prompt = \"こんにちは。元気、ですかー？ふふっ、私は……ちゃんと元気だよ！\"\n",
        "\n",
        "!python slice.py -i {input_dir} --model_name {model_name}\n",
        "!python transcribe.py --model_name {model_name} --initial_prompt {initial_prompt} --use_hf_whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7vEWewoAWvS"
      },
      "source": [
        "成功したらそのまま3へ進んでください"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3AC-3zpAWvS"
      },
      "source": [
        "### 2.2 音声ファイルと書き起こしデータがすでにある場合\n",
        "\n",
        "指示に従って適切にデータセットを配置してください。\n",
        "\n",
        "次のセルを実行して、学習データをいれるフォルダ（1で設定した`dataset_root`）を作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esCNJl704h52"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(dataset_root, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaDgJCjCAWvT"
      },
      "source": [
        "まず音声データと、書き起こしテキストを用意してください。\n",
        "\n",
        "それを次のように配置します。\n",
        "```\n",
        "├── Data/\n",
        "│   ├── {モデルの名前}\n",
        "│   │   ├── esd.list\n",
        "│   │   ├── raw/\n",
        "│   │   │   ├── foo.wav\n",
        "│   │   │   ├── bar.mp3\n",
        "│   │   │   ├── style1/\n",
        "│   │   │   │   ├── baz.wav\n",
        "│   │   │   │   ├── qux.wav\n",
        "│   │   │   ├── style2/\n",
        "│   │   │   │   ├── corge.wav\n",
        "│   │   │   │   ├── grault.wav\n",
        "...\n",
        "```\n",
        "\n",
        "### 配置の仕方\n",
        "- 上のように配置すると、`style1/`と`style2/`フォルダの内部（直下以外も含む）に入っている音声ファイルたちから、自動的にデフォルトスタイルに加えて`style1`と`style2`というスタイルが作成されます\n",
        "- 特にスタイルを作る必要がない場合や、スタイル分類機能等でスタイルを作る場合は、`raw/`フォルダ直下に全てを配置してください。このように`raw/`のサブディレクトリの個数が0または1の場合は、スタイルはデフォルトスタイルのみが作成されます。\n",
        "- 音声ファイルのフォーマットはwav形式以外にもmp3等の多くの音声ファイルに対応しています\n",
        "\n",
        "### 書き起こしファイル`esd.list`\n",
        "\n",
        "`Data/{モデルの名前}/esd.list` ファイルには、以下のフォーマットで各音声ファイルの情報を記述してください。\n",
        "\n",
        "\n",
        "```\n",
        "path/to/audio.wav(wavファイル以外でもこう書く)|{話者名}|{言語ID、ZHかJPかEN}|{書き起こしテキスト}\n",
        "```\n",
        "\n",
        "- ここで、最初の`path/to/audio.wav`は、`raw/`からの相対パスです。つまり、`raw/foo.wav`の場合は`foo.wav`、`raw/style1/bar.wav`の場合は`style1/bar.wav`となります。\n",
        "- 拡張子がwavでない場合でも、`esd.list`には`wav`と書いてください、つまり、`raw/bar.mp3`の場合でも`bar.wav`と書いてください。\n",
        "\n",
        "\n",
        "例：\n",
        "```\n",
        "foo.wav|hanako|JP|こんにちは、元気ですか？\n",
        "bar.wav|taro|JP|はい、聞こえています……。何か用ですか？\n",
        "style1/baz.wav|hanako|JP|今日はいい天気ですね。\n",
        "style1/qux.wav|taro|JP|はい、そうですね。\n",
        "...\n",
        "english_teacher.wav|Mary|EN|How are you? I'm fine, thank you, and you?\n",
        "...\n",
        "```\n",
        "もちろん日本語話者の単一話者データセットでも構いません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r85-W20ECcr"
      },
      "source": [
        "## 3. 学習の前処理\n",
        "\n",
        "次に学習の前処理を行います。必要なパラメータをここで指定します。次のセルに設定等を入力して実行してください。「～～かどうか」は`True`もしくは`False`を指定してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CXR7kjuF5GlE"
      },
      "outputs": [],
      "source": [
        "# 上でつけたフォルダの名前`Data/{model_name}/`\n",
        "model_name = \"aoyama\"\n",
        "\n",
        "# JP-Extra （日本語特化版）を使うかどうか。日本語の能力が向上する代わりに英語と中国語は使えなくなります。\n",
        "use_jp_extra = True\n",
        "\n",
        "# 学習のバッチサイズ。VRAMのはみ出具合に応じて調整してください。\n",
        "batch_size = 4\n",
        "\n",
        "# 学習のエポック数（データセットを合計何周するか）。\n",
        "# 100で多すぎるほどかもしれませんが、もっと多くやると質が上がるのかもしれません。\n",
        "epochs = 100\n",
        "\n",
        "# 保存頻度。何ステップごとにモデルを保存するか。分からなければデフォルトのままで。\n",
        "save_every_steps = 1000\n",
        "\n",
        "# 音声ファイルの音量を正規化するかどうか\n",
        "normalize = False\n",
        "\n",
        "# 音声ファイルの開始・終了にある無音区間を削除するかどうか\n",
        "trim = False\n",
        "\n",
        "# 読みのエラーが出た場合にどうするか。\n",
        "# \"raise\"ならテキスト前処理が終わったら中断、\"skip\"なら読めない行は学習に使わない、\"use\"なら無理やり使う\n",
        "yomi_error = \"skip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFZdLTtpAWvT"
      },
      "source": [
        "上のセルが実行されたら、次のセルを実行して学習の前処理を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMVaOIPLabV5",
        "outputId": "c9f07a18-8652-4d94-c92f-0c4f10a48f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09-17 10:04:54 |  INFO  | train.py:72 | Step 1: start initialization...\n",
            "model_name: aoyama, batch_size: 4, epochs: 100, save_every_steps: 1000, freeze_ZH_bert: False, freeze_JP_bert: False, freeze_EN_bert: False, freeze_style: False, freeze_decoder: False, use_jp_extra: True\n",
            "09-17 10:05:01 |SUCCESS | train.py:132 | Step 1: initialization finished.\n",
            "09-17 10:05:01 |  INFO  | train.py:137 | Step 2: start resampling...\n",
            "09-17 10:05:01 |  INFO  | subprocess.py:23 | Running: resample.py -i /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/raw -o /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/wavs --num_processes 2 --sr 44100\n",
            "09-17 10:05:08 |SUCCESS | subprocess.py:38 | Success: resample.py -i /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/raw -o /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/wavs --num_processes 2 --sr 44100\n",
            "09-17 10:05:08 |SUCCESS | train.py:163 | Step 2: resampling finished.\n",
            "09-17 10:05:08 |  INFO  | train.py:170 | Step 3: start preprocessing text...\n",
            "09-17 10:05:08 |  INFO  | subprocess.py:23 | Running: preprocess_text.py --config-path /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/config.json --transcription-path /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/esd.list --train-path /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/train.list --val-path /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/val.list --val-per-lang 0 --yomi_error skip --correct_path --use_jp_extra\n",
            "09-17 10:05:23 |WARNING | subprocess.py:36 | Warning: preprocess_text.py --config-path /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/config.json --transcription-path /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/esd.list --train-path /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/train.list --val-path /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/val.list --val-per-lang 0 --yomi_error skip --correct_path --use_jp_extra\n",
            "2025-09-17 10:05:18.484235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758103518.504550   18231 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758103518.512087   18231 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758103518.527914   18231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103518.527936   18231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103518.527938   18231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103518.527940   18231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-17 10:05:18.532717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "09-17 10:05:23 |WARNING | train.py:205 | Step 3: preprocessing text finished with stderr.\n",
            "09-17 10:05:23 |  INFO  | train.py:215 | Step 4: start bert_gen...\n",
            "09-17 10:05:23 |  INFO  | subprocess.py:23 | Running: bert_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/config.json\n",
            "09-17 10:11:34 |WARNING | subprocess.py:36 | Warning: bert_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/config.json\n",
            "2025-09-17 10:05:31.649718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758103531.687626   18340 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758103531.699419   18340 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758103531.728765   18340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103531.728812   18340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103531.728821   18340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103531.728828   18340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-17 10:05:31.737458: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "09-17 10:11:34 |WARNING | train.py:224 | Step 4: bert_gen finished with stderr.\n",
            "09-17 10:11:34 |  INFO  | train.py:234 | Step 5: start style_gen...\n",
            "09-17 10:11:34 |  INFO  | subprocess.py:23 | Running: style_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/config.json --num_processes 2\n",
            "09-17 10:11:51 |WARNING | subprocess.py:36 | Warning: style_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/config.json --num_processes 2\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/util.py:182: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  sep=\"\\s+\",\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/util.py:216: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  sep=\"\\s+\",\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/util.py:253: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  sep=\"\\s+\",\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/util.py:284: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  data = pd.read_csv(file_uem, names=names, dtype=dtype, sep=\"\\s+\")\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/util.py:309: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  data = pd.read_csv(path, names=names, dtype=dtype, sep=\"\\s+\")\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/loader.py:93: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  file_trial, sep=\"\\s+\", names=[\"reference\", \"uri1\", \"uri2\"]\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/loader.py:292: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  ctm, names=names, dtype=dtype, sep=\"\\s+\"\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/database/loader.py:357: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  mapping, names=names, dtype=dtype, sep=\"\\s+\"\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  torchaudio.list_audio_backends()\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
            "It can be re-enabled by calling\n",
            "   >>> import torch\n",
            "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
            "   >>> torch.backends.cudnn.allow_tf32 = True\n",
            "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "\n",
            "09-17 10:11:51 |WARNING | train.py:252 | Step 5: style_gen finished with stderr.\n",
            "09-17 10:11:51 |SUCCESS | train.py:321 | Success: All preprocess finished!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 'Success: 全ての前処理が完了しました。ターミナルを確認しておかしいところがないか確認するのをおすすめします。')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "from gradio_tabs.train import preprocess_all\n",
        "from style_bert_vits2.nlp.japanese import pyopenjtalk_worker\n",
        "\n",
        "\n",
        "pyopenjtalk_worker.initialize_worker()\n",
        "\n",
        "preprocess_all(\n",
        "    model_name=model_name,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    save_every_steps=save_every_steps,\n",
        "    num_processes=2,\n",
        "    normalize=normalize,\n",
        "    trim=trim,\n",
        "    freeze_EN_bert=False,\n",
        "    freeze_JP_bert=False,\n",
        "    freeze_ZH_bert=False,\n",
        "    freeze_style=False,\n",
        "    freeze_decoder=False,\n",
        "    use_jp_extra=use_jp_extra,\n",
        "    val_per_lang=0,\n",
        "    log_interval=200,\n",
        "    yomi_error=yomi_error,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVhwI5C-AWvT"
      },
      "source": [
        "## 4. 学習\n",
        "\n",
        "前処理が正常に終わったら、学習を行います。次のセルを実行すると学習が始まります。\n",
        "\n",
        "学習の結果は、上で指定した`save_every_steps`の間隔で、Google Driveの中の`Style-Bert-VITS2/Data/{モデルの名前}/model_assets/`フォルダに保存されます。\n",
        "\n",
        "このフォルダをダウンロードし、ローカルのStyle-Bert-VITS2の`model_assets`フォルダに上書きすれば、学習結果を使うことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "laieKrbEb6Ij"
      },
      "outputs": [],
      "source": [
        "# 上でつけたモデル名を入力。学習を途中からする場合はきちんとモデルが保存されているフォルダ名を入力。\n",
        "model_name = \"aoyama\"\n",
        "\n",
        "\n",
        "import yaml\n",
        "from gradio_tabs.train import get_path\n",
        "\n",
        "paths = get_path(model_name)\n",
        "dataset_path = str(paths.dataset_path)\n",
        "config_path = str(paths.config_path)\n",
        "\n",
        "with open(\"default_config.yml\", \"r\", encoding=\"utf-8\") as f:\n",
        "    yml_data = yaml.safe_load(f)\n",
        "yml_data[\"model_name\"] = model_name\n",
        "with open(\"config.yml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.dump(yml_data, f, allow_unicode=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqGeHNabAWvT",
        "outputId": "6c65fc9f-2b5a-411b-bb9a-dc9b17613426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-17 10:12:49.246912: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758103969.266621   20272 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758103969.272662   20272 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758103969.288306   20272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103969.288330   20272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103969.288334   20272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758103969.288339   20272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-17 10:12:49.293297: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "09-17 10:12:58 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 0\n",
            "09-17 10:12:58 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config localhost\n",
            "09-17 10:12:58 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 10086\n",
            "09-17 10:12:58 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 0\n",
            "09-17 10:12:58 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 1\n",
            "09-17 10:12:58 |  INFO  | train_ms_jp_extra.py:120 | Loading environment variables \n",
            "MASTER_ADDR: localhost,\n",
            "MASTER_PORT: 10086,\n",
            "WORLD_SIZE: 1,\n",
            "RANK: 0,\n",
            "LOCAL_RANK: 0\n",
            "09-17 10:12:58 |  INFO  | default_style.py:54 | At least 2 subdirectories are required for generating style vectors with respect to them, found 1.\n",
            "09-17 10:12:58 |  INFO  | default_style.py:57 | Generating only neutral style vector instead.\n",
            "09-17 10:12:58 |  INFO  | default_style.py:28 | Saved mean style vector to /content/drive/MyDrive/Style-Bert-VITS2/model_assets/aoyama\n",
            "09-17 10:12:58 |  INFO  | default_style.py:36 | Saved style config to /content/drive/MyDrive/Style-Bert-VITS2/model_assets/aoyama/config.json\n",
            "09-17 10:12:58 |WARNING | __init__.py:247 | /content/Style-Bert-VITS2/style_bert_vits2/models/utils is not a git repository, therefore hash value comparison will be ignored.\n",
            "09-17 10:12:58 |  INFO  | data_utils.py:69 | Init dataset...\n",
            "100% 11/11 [00:00<00:00, 2751.18it/s]\n",
            "09-17 10:12:58 |  INFO  | data_utils.py:84 | skipped: 0, total: 11\n",
            "09-17 10:12:58 |  INFO  | data_utils.py:374 | Bucket warning \n",
            "09-17 10:12:58 |  INFO  | data_utils.py:348 | Bucket info: [4, 4, 4, 4]\n",
            "09-17 10:12:58 |  INFO  | data_utils.py:69 | Init dataset...\n",
            "0it [00:00, ?it/s]\n",
            "09-17 10:12:58 |  INFO  | data_utils.py:84 | skipped: 0, total: 0\n",
            "09-17 10:12:58 |  INFO  | train_ms_jp_extra.py:287 | Using noise scaled MAS for VITS2\n",
            "09-17 10:13:01 |WARNING | safetensors.py:43 | Missing key: enc_p.style_proj.weight\n",
            "09-17 10:13:01 |WARNING | safetensors.py:43 | Missing key: enc_p.style_proj.bias\n",
            "09-17 10:13:01 |WARNING | safetensors.py:43 | Missing key: emb_g.weight\n",
            "09-17 10:13:01 |  INFO  | safetensors.py:49 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/models/G_0.safetensors'\n",
            "09-17 10:13:01 |  INFO  | safetensors.py:49 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/models/D_0.safetensors'\n",
            "09-17 10:13:01 |  INFO  | safetensors.py:49 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/models/WD_0.safetensors'\n",
            "09-17 10:13:01 |  INFO  | train_ms_jp_extra.py:505 | Loaded the pretrained models.\n",
            "09-17 10:13:04 |  INFO  | train_ms_jp_extra.py:553 | Start training.\n",
            "Epoch 100(75%)/100: 100%|##########| 400/400 [11:15<00:00,  1.65s/it]09-17 10:24:19 |  INFO  | checkpoints.py:111 | Saving model and optimizer state at iteration 100 to /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/models/G_400.pth\n",
            "09-17 10:24:26 |  INFO  | checkpoints.py:111 | Saving model and optimizer state at iteration 100 to /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/models/D_400.pth\n",
            "09-17 10:24:38 |  INFO  | checkpoints.py:111 | Saving model and optimizer state at iteration 100 to /content/drive/MyDrive/Style-Bert-VITS2/Data/aoyama/models/WD_400.pth\n",
            "09-17 10:24:38 |  INFO  | safetensors.py:90 | Saved safetensors to /content/drive/MyDrive/Style-Bert-VITS2/model_assets/aoyama/aoyama_e100_s400.safetensors\n",
            "Epoch 100(75%)/100: 100%|##########| 400/400 [11:39<00:00,  1.75s/it]\n",
            "[rank0]:[W917 10:24:44.898411648 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "# 日本語特化版を「使う」場合\n",
        "!python train_ms_jp_extra.py --config {config_path} --model {dataset_path} --assets_root {assets_root}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVbjh-WPAWvU"
      },
      "outputs": [],
      "source": [
        "# 日本語特化版を「使わない」場合\n",
        "!python train_ms.py --config {config_path} --model {dataset_path} --assets_root {assets_root}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7g0hrdeP1Tl",
        "outputId": "f3dc43fe-60b4-4ecc-8b00-42d3077da990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-17 10:25:53.745415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758104753.766737   23625 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758104753.773331   23625 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758104753.789637   23625 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758104753.789663   23625 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758104753.789667   23625 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758104753.789670   23625 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-17 10:25:53.794578: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "09-17 10:25:57 | DEBUG  | __init__.py:130 | pyopenjtalk worker server started\n",
            "theme_schema%401.2.2.json: 12.0kB [00:00, 45.8MB/s]\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://c12f1ce9eaa36432c8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "09-17 10:28:12 | DEBUG  | inference.py:268 | Null models setting: {}\n",
            "09-17 10:28:12 |  INFO  | tts_model.py:410 | Start generating audio data from text:\n",
            "こんにちは、初めまして。あなたの名前はなんていうの？\n",
            "09-17 10:28:12 |  INFO  | infer.py:26 | Using JP-Extra model\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "09-17 10:28:13 |  INFO  | safetensors.py:51 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/model_assets/aoyama/aoyama_e100_s400.safetensors' (iteration 100)\n",
            "09-17 10:28:13 |  INFO  | tts_model.py:152 | Model loaded successfully from /content/drive/MyDrive/Style-Bert-VITS2/model_assets/aoyama/aoyama_e100_s400.safetensors to \"cuda\" device (1.41s)\n",
            "09-17 10:28:13 |  INFO  | tts_model.py:196 | Null models merged successfully (1.41s)\n",
            "09-17 10:28:13 |  INFO  | bert_models.py:178 | Loaded the JP BERT tokenizer from /content/Style-Bert-VITS2/bert/deberta-v2-large-japanese-char-wwm\n"
          ]
        }
      ],
      "source": [
        "# 学習結果を試す・マージ・スタイル分けはこちらから\n",
        "!python app.py --share"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APdcn1Qg5Ut-"
      },
      "outputs": [],
      "source": [
        "# ONNX変換は、変換したいsafetensorsファイルを指定してこのセルを実行してください。\n",
        "!python convert_onnx.py --model \"Data/your_model/aoyama_e100_s400.safetensors\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}